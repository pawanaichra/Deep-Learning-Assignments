{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** `Pawan Kumar Aichra`<br/>\n",
    "** Roll Number: ** `15HS20026`<br/>\n",
    "** Department: ** `Humanities and Social Sciences`<br/>\n",
    "** Email: ** ` aichrapawan@gmail.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "# print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.standard_normal(size=(512, 784))\n",
    "W2 = np.random.standard_normal(size=(256, 512))\n",
    "W3 = np.random.standard_normal(size=(10, 256))\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "    x = x - np.max(x, axis=0)\n",
    "    x = np.exp(x)\n",
    "    x = x/np.sum(x, axis=0)\n",
    "    return np.transpose(x)     \n",
    "    ####################################################################################\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 16.1205 \n",
      " Epoch: 1, iteration: 937, Loss: 14.6202 \n",
      " Epoch: 2, iteration: 1874, Loss: 3.1153 \n",
      " Epoch: 3, iteration: 2811, Loss: 2.3026 \n",
      " Epoch: 4, iteration: 3748, Loss: 2.3026 \n",
      " Epoch: 5, iteration: 4685, Loss: 2.3026 \n",
      " Epoch: 6, iteration: 5622, Loss: 2.3026 \n",
      " Epoch: 7, iteration: 6559, Loss: 2.3026 \n",
      " Epoch: 8, iteration: 7496, Loss: 2.3026 \n",
      " Epoch: 9, iteration: 8433, Loss: 2.3026 \n",
      " Epoch: 10, iteration: 9370, Loss: 2.3026 \n",
      " Epoch: 11, iteration: 10307, Loss: 2.3026 \n",
      " Epoch: 12, iteration: 11244, Loss: 2.3026 \n",
      " Epoch: 13, iteration: 12181, Loss: 2.3026 \n",
      " Epoch: 14, iteration: 13118, Loss: 2.3026 \n",
      " Epoch: 15, iteration: 14055, Loss: 2.3026 \n",
      " Epoch: 16, iteration: 14992, Loss: 2.3026 \n",
      " Epoch: 17, iteration: 15929, Loss: 2.3026 \n",
      " Epoch: 18, iteration: 16866, Loss: 2.3026 \n",
      " Epoch: 19, iteration: 17803, Loss: 2.3026 \n",
      " Epoch: 20, iteration: 18740, Loss: 2.3026 \n",
      " Epoch: 21, iteration: 19677, Loss: 2.3026 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 20000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 = np.matmul(W1, x_batchinput.T)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1, 0)\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.matmul(W2, h1)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2, 0)\n",
    "    #implement linear output layer\n",
    "    a3 = np.matmul(W3, h2)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "\n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    y_batchinput = y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    grad_softmax_score = np.mean(-(np.transpose(y_batchinput)-np.transpose(softmax_score)), axis=1)\n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = np.sum(np.reshape(grad_softmax_score, (-1,1,1))*h2, axis=2)\n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 = np.matmul(grad_softmax_score, W3)\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = np.maximum(grad_h2, 0)\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = np.sum(np.reshape(grad_a2, (-1,1,1))*h1, axis=2)\n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = np.matmul(grad_a2, W2)\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = np.maximum(grad_h1, 0)\n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = np.sum(np.reshape(grad_a1, (-1,1,1))*(x_batchinput.T), axis=2)\n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 9.80 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    a1 = np.matmul(W1, x_batchinput.T)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1, 0)\n",
    "    #  implement 2nd hidden layer\n",
    "    a2 = np.matmul(W2, h1)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2, 0)\n",
    "    #implement linear output layer\n",
    "    a3 = np.matmul(W3, h2)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
